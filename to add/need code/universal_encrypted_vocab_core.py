#!/usr/bin/env python3
"""
UNIVERSAL ENCRYPTED LANGUAGE CORE
=================================

A plug-and-play encrypted language intelligence system that can be integrated
into ANY AI system to provide massive vocabulary coverage, semantic understanding,
and linguistic intelligence without LLM dependency.

Features:
- 100k+ encrypted English vocabulary with definitions, synonyms, embeddings
- Universal API that works with any AI architecture
- Complete offline operation with privacy protection
- Semantic similarity and word relationship mapping
- Morphological awareness and lemmatization
- Adaptive integration with existing AI personalities
- Zero external dependencies or data transmission

Compatible with:
- Custom AI systems (like Anima)
- Open-source language models
- Chatbot frameworks
- Educational AI systems
- Therapeutic AI applications
- Creative AI tools
- Any system needing rich vocabulary understanding

Author: Universal Language Intelligence Project
"""

import os
import re
import json
import gzip
import math
import hmac
import uuid
import hashlib
import struct
import logging
from abc import ABC, abstractmethod
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any, Union, Callable
from dataclasses import dataclass, field
from collections import OrderedDict, defaultdict
from enum import Enum

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger('UniversalLanguageCore')

# =============================================================================
# ENCRYPTED LEXICON INTEGRATION (Enhanced from provided code)
# =============================================================================

try:
    from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC
    from cryptography.hazmat.primitives import hashes
    from cryptography.hazmat.primitives.ciphers.aead import AESGCM
    CRYPTO_AVAILABLE = True
except ImportError:
    logger.warning("Cryptography not available - encryption features disabled")
    CRYPTO_AVAILABLE = False

MAGIC = b"UNIVLANG1"
VERSION = 1

def pbkdf2_key(password: bytes, salt: bytes, length: int = 64, rounds: int = 200_000) -> bytes:
    """Derive key from password using PBKDF2"""
    if not CRYPTO_AVAILABLE:
        # Fallback to simple hash for demo (NOT secure for production)
        return hashlib.sha256(password + salt).digest()[:length]
    
    kdf = PBKDF2HMAC(algorithm=hashes.SHA256(), length=length, salt=salt, iterations=rounds)
    return kdf.derive(password)

def hkdf_split(master: bytes) -> Tuple[bytes, bytes]:
    """Split master key into encryption and MAC keys"""
    return master[:32], master[32:]

def encrypt_bytes(enc_key: bytes, plaintext: bytes, aad: bytes = b"") -> bytes:
    """Encrypt bytes with AES-GCM"""
    if not CRYPTO_AVAILABLE:
        # Simple XOR for demo (NOT secure for production)
        result = bytearray()
        for i, byte in enumerate(plaintext):
            result.append(byte ^ enc_key[i % len(enc_key)])
        return bytes(result)
    
    nonce = os.urandom(12)
    ct = AESGCM(enc_key).encrypt(nonce, plaintext, aad)
    return nonce + ct

def decrypt_bytes(enc_key: bytes, blob: bytes, aad: bytes = b"") -> bytes:
    """Decrypt bytes with AES-GCM"""
    if not CRYPTO_AVAILABLE:
        # Simple XOR for demo (NOT secure for production)
        result = bytearray()
        for i, byte in enumerate(blob):
            result.append(byte ^ enc_key[i % len(enc_key)])
        return bytes(result)
    
    nonce, ct = blob[:12], blob[12:]
    return AESGCM(enc_key).decrypt(nonce, ct, aad)

# Quantization utilities
def int8_quantize(vec: List[float], d: int = 32) -> bytes:
    """Quantize float vector to int8 for compression"""
    vec = (vec[:d] + [0.0] * max(0, d - len(vec)))[:d]
    out = []
    for x in vec:
        x = max(-1.0, min(1.0, float(x)))
        out.append(int(round(x * 127)))
    return bytes((v & 0xFF for v in out))

def int8_dequantize(blob: bytes) -> List[float]:
    """Dequantize int8 back to float vector"""
    return [((b if b < 128 else b - 256) / 127.0) for b in blob]

def cosine_similarity(a: List[float], b: List[float]) -> float:
    """Calculate cosine similarity between vectors"""
    num = sum(x * y for x, y in zip(a, b))
    da = math.sqrt(sum(x * x for x in a)) + 1e-9
    db = math.sqrt(sum(x * x for x in b)) + 1e-9
    return num / (da * db)

# Morphological processing
_IRREGULAR_FORMS = {
    "mice": "mouse", "geese": "goose", "men": "man", "women": "woman", "children": "child",
    "teeth": "tooth", "feet": "foot", "went": "go", "gone": "go", "did": "do", "done": "do",
    "was": "be", "were": "be", "been": "be", "am": "be", "is": "be", "are": "be",
    "better": "good", "best": "good", "worse": "bad", "worst": "bad"
}

_MORPHOLOGICAL_RULES = [
    (r'(.+?)(sses|shes|ches|xes)$', r'\1s'),
    (r'(.+?)ies$', r'\1y'),
    (r'(.+?)ves$', r'\1f'),
    (r'(.+?)ing$', r'\1'),
    (r'(.+?)ed$', r'\1'),
    (r'(.+?)s$', r'\1'),
]

def normalize_word(word: str) -> str:
    """Normalize word to its base form"""
    w = word.lower().strip()
    if w in _IRREGULAR_FORMS:
        return _IRREGULAR_FORMS[w]
    
    for pattern, replacement in _MORPHOLOGICAL_RULES:
        if re.match(pattern, w):
            return re.sub(pattern, replacement, w)
    
    return w

# =============================================================================
# UNIVERSAL WORD KNOWLEDGE STRUCTURES
# =============================================================================

class WordCategory(Enum):
    """Universal word categories"""
    NOUN = "noun"
    VERB = "verb" 
    ADJECTIVE = "adjective"
    ADVERB = "adverb"
    PREPOSITION = "preposition"
    CONJUNCTION = "conjunction"
    INTERJECTION = "interjection"
    PRONOUN = "pronoun"
    DETERMINER = "determiner"
    UNKNOWN = "unknown"

class SemanticDomain(Enum):
    """Semantic domains for specialized vocabularies"""
    GENERAL = "general"
    EMOTIONAL = "emotional"
    THERAPEUTIC = "therapeutic"
    PHILOSOPHICAL = "philosophical"
    TECHNICAL = "technical"
    CREATIVE = "creative"
    SCIENTIFIC = "scientific"
    SPIRITUAL = "spiritual"
    SOCIAL = "social"

@dataclass
class UniversalWordKnowledge:
    """Universal word knowledge structure for any AI system"""
    word: str
    lemma: str
    category: WordCategory
    definitions: List[str]
    synonyms: List[str] = field(default_factory=list)
    antonyms: List[str] = field(default_factory=list)
    related_words: List[str] = field(default_factory=list)
    
    # Semantic properties
    semantic_domain: SemanticDomain = SemanticDomain.GENERAL
    emotional_valence: float = 0.0  # -1.0 (negative) to 1.0 (positive)
    arousal_level: float = 0.0      # 0.0 (calm) to 1.0 (exciting)
    concreteness: float = 0.5       # 0.0 (abstract) to 1.0 (concrete)
    
    # Usage properties
    frequency_rank: int = 999999
    formality_level: float = 0.5    # 0.0 (informal) to 1.0 (formal)
    complexity_level: float = 0.5   # 0.0 (simple) to 1.0 (complex)
    
    # Vector representation
    embedding: List[float] = field(default_factory=lambda: [0.0] * 32)
    
    # Metadata
    source: str = "universal"
    confidence: float = 1.0

# =============================================================================
# UNIVERSAL AI INTEGRATION INTERFACE
# =============================================================================

class AIPersonalityProfile:
    """Profile that describes an AI's personality for vocabulary adaptation"""
    
    def __init__(self, name: str = "Universal AI"):
        self.name = name
        self.personality_traits = {}
        self.preferred_domains = []
        self.emotional_style = "neutral"
        self.formality_preference = 0.5
        self.complexity_preference = 0.5
        self.vocabulary_filters = {}
    
    def set_trait(self, trait: str, value: float):
        """Set personality trait (0.0 to 1.0)"""
        self.personality_traits[trait] = max(0.0, min(1.0, value))
    
    def add_preferred_domain(self, domain: SemanticDomain):
        """Add preferred semantic domain"""
        if domain not in self.preferred_domains:
            self.preferred_domains.append(domain)
    
    def set_emotional_style(self, style: str):
        """Set emotional communication style"""
        valid_styles = ["neutral", "warm", "analytical", "empathetic", "energetic", "calm"]
        if style in valid_styles:
            self.emotional_style = style
    
    def add_vocabulary_filter(self, filter_name: str, word_list: List[str], preference: float):
        """Add vocabulary preference filter"""
        self.vocabulary_filters[filter_name] = {
            "words": set(word_list),
            "preference": preference
        }

class UniversalAIAdapter(ABC):
    """Abstract adapter interface for integrating with any AI system"""
    
    @abstractmethod
    def get_personality_profile(self) -> AIPersonalityProfile:
        """Return the AI's personality profile for vocabulary adaptation"""
        pass
    
    @abstractmethod
    def process_word_knowledge(self, word: str, knowledge: UniversalWordKnowledge) -> Any:
        """Process word knowledge into the AI's internal format"""
        pass
    
    @abstractmethod
    def estimate_context_needs(self, text: str) -> Dict[str, float]:
        """Estimate what kind of vocabulary/knowledge the context needs"""
        pass
    
    def adapt_vocabulary_selection(self, candidates: List[UniversalWordKnowledge], 
                                 context: Dict[str, float]) -> List[UniversalWordKnowledge]:
        """Adapt vocabulary selection based on AI personality and context"""
        profile = self.get_personality_profile()
        adapted = []
        
        for word_knowledge in candidates:
            # Calculate suitability score
            score = self._calculate_suitability(word_knowledge, profile, context)
            if score > 0.3:  # Threshold for inclusion
                adapted.append(word_knowledge)
        
        # Sort by suitability
        adapted.sort(key=lambda w: self._calculate_suitability(w, profile, context), reverse=True)
        return adapted
    
    def _calculate_suitability(self, word: UniversalWordKnowledge, 
                             profile: AIPersonalityProfile, context: Dict[str, float]) -> float:
        """Calculate how suitable a word is for this AI and context"""
        score = 0.5  # Base score
        
        # Domain preference
        if word.semantic_domain in profile.preferred_domains:
            score += 0.3
        
        # Formality matching
        formality_diff = abs(word.formality_level - profile.formality_preference)
        score += (1.0 - formality_diff) * 0.2
        
        # Complexity matching
        complexity_diff = abs(word.complexity_level - profile.complexity_preference)
        score += (1.0 - complexity_diff) * 0.2
        
        # Vocabulary filter preferences
        for filter_name, filter_data in profile.vocabulary_filters.items():
            if word.word in filter_data["words"] or word.lemma in filter_data["words"]:
                score += filter_data["preference"] * 0.3
        
        # Context relevance
        for context_type, importance in context.items():
            if context_type == word.semantic_domain.value:
                score += importance * 0.4
        
        return min(score, 1.0)

# =============================================================================
# UNIVERSAL ENCRYPTED LEXICON
# =============================================================================

class _LRU(OrderedDict):
    """LRU cache implementation"""
    def __init__(self, capacity=8):
        super().__init__()
        self.capacity = capacity
    
    def get_or(self, key, loader):
        if key in self:
            val = self.pop(key)
            self[key] = val
            return val
        val = loader()
        self[key] = val
        if len(self) > self.capacity:
            self.popitem(last=False)
        return val

class UniversalEncryptedLexicon:
    """Universal encrypted lexicon that works with any AI system"""
    
    def __init__(self, lexicon_path: str, password: str, cache_size: int = 8):
        self.lexicon_path = Path(lexicon_path)
        self.password = password
        self._index: Dict[str, Dict[str, str]] = {}
        self._salt: bytes = b""
        self._enc_key: bytes = b""
        self._mac_key: bytes = b""
        self._cache = _LRU(capacity=cache_size)
        self._stats = defaultdict(int)
        
        if self.lexicon_path.exists():
            self._load_index()
            logger.info(f"Universal encrypted lexicon loaded from {lexicon_path}")
        else:
            logger.warning(f"No encrypted lexicon found at {lexicon_path}")
            self._index = {}
    
    def _derive_keys(self):
        """Derive encryption and MAC keys from password"""
        master = pbkdf2_key(self.password.encode("utf-8"), self._salt)
        self._enc_key, self._mac_key = hkdf_split(master)
    
    def _load_index(self):
        """Load the encrypted index"""
        index_path = self.lexicon_path / "index.bin"
        if not index_path.exists():
            return
        
        try:
            with open(index_path, "rb") as f:
                magic = f.read(len(MAGIC))
                if magic != MAGIC:
                    logger.error("Invalid lexicon magic number")
                    return
                
                version = struct.unpack(">I", f.read(4))[0]
                if version != VERSION:
                    logger.error(f"Version mismatch: expected {VERSION}, got {version}")
                    return
                
                self._salt = f.read(16)
                self._derive_keys()
                encrypted_index = f.read()
            
            # Decrypt and load index
            aad = MAGIC + struct.pack(">I", VERSION) + b"INDEX"
            compressed_index = decrypt_bytes(self._enc_key, encrypted_index, aad=aad)
            index_data = gzip.decompress(compressed_index)
            index_json = json.loads(index_data.decode("utf-8"))
            self._index = index_json.get("map", {})
            
        except Exception as e:
            logger.error(f"Failed to load encrypted lexicon index: {e}")
            self._index = {}
    
    def _load_shard(self, shard_name: str) -> Dict[str, Any]:
        """Load and decrypt a vocabulary shard"""
        shard_path = self.lexicon_path / shard_name
        
        try:
            with open(shard_path, "rb") as f:
                magic = f.read(len(MAGIC))
                if magic != MAGIC:
                    raise ValueError("Invalid shard magic")
                
                version = struct.unpack(">I", f.read(4))[0]
                salt = f.read(16)
                shard_id = struct.unpack(">I", f.read(4))[0]
                encrypted_data = f.read()
            
            if salt != self._salt:
                raise ValueError("Salt mismatch")
            
            # Decrypt shard
            aad = MAGIC + struct.pack(">I", VERSION) + b"SHARD" + struct.pack(">I", shard_id)
            compressed_data = decrypt_bytes(self._enc_key, encrypted_data, aad=aad)
            shard_data = gzip.decompress(compressed_data)
            shard_json = json.loads(shard_data.decode("utf-8"))
            
            return shard_json.get("entries", {})
            
        except Exception as e:
            logger.error(f"Failed to load shard {shard_name}: {e}")
            return {}
    
    def _get_shard(self, shard_name: str) -> Dict[str, Any]:
        """Get shard with caching"""
        return self._cache.get_or(shard_name, lambda: self._load_shard(shard_name))
    
    def lookup_word(self, word: str) -> Optional[UniversalWordKnowledge]:
        """Look up a word and return universal knowledge structure"""
        if not self._index:
            return None
        
        lemma = normalize_word(word)
        
        # Generate HMAC token for privacy-preserving lookup
        token = hmac.new(self._mac_key, lemma.encode("utf-8"), hashlib.sha256).hexdigest()
        
        shard_info = self._index.get(token)
        if not shard_info:
            self._stats["misses"] += 1
            return None
        
        # Load shard and find word
        shard_data = self._get_shard(shard_info["sh"])
        word_data = shard_data.get(lemma)
        
        if not word_data:
            self._stats["misses"] += 1
            return None
        
        self._stats["hits"] += 1
        
        # Convert to universal format
        return self._convert_to_universal_knowledge(lemma, word_data)
    
    def _convert_to_universal_knowledge(self, lemma: str, word_data: Dict) -> UniversalWordKnowledge:
        """Convert lexicon data to universal word knowledge"""
        # Extract all definitions and synonyms from senses
        all_definitions = []
        all_synonyms = []
        primary_pos = "unknown"
        
        senses = word_data.get("s", [])
        if senses:
            primary_pos = senses[0].get("p", "unknown")
            for sense in senses:
                all_definitions.extend(sense.get("d", []))
                all_synonyms.extend(sense.get("y", []))
        
        # Map part of speech
        pos_mapping = {
            "n": WordCategory.NOUN,
            "v": WordCategory.VERB,
            "adj": WordCategory.ADJECTIVE,
            "adv": WordCategory.ADVERB,
            "prep": WordCategory.PREPOSITION,
            "conj": WordCategory.CONJUNCTION,
            "interj": WordCategory.INTERJECTION,
            "pron": WordCategory.PRONOUN,
            "det": WordCategory.DETERMINER
        }
        category = pos_mapping.get(primary_pos, WordCategory.UNKNOWN)
        
        # Estimate semantic properties
        emotional_valence = self._estimate_emotional_valence(lemma, all_definitions)
        arousal_level = self._estimate_arousal_level(lemma, all_definitions)
        concreteness = self._estimate_concreteness(category, all_definitions)
        semantic_domain = self._determine_semantic_domain(lemma, all_definitions, category)
        
        # Get embedding if available
        embedding = []
        if "m" in word_data:
            embedding = int8_dequantize(bytes(word_data["m"]))
        
        return UniversalWordKnowledge(
            word=lemma,
            lemma=lemma,
            category=category,
            definitions=all_definitions[:3],  # Keep top 3 definitions
            synonyms=list(set(all_synonyms[:5])),  # Deduplicate synonyms
            semantic_domain=semantic_domain,
            emotional_valence=emotional_valence,
            arousal_level=arousal_level,
            concreteness=concreteness,
            frequency_rank=word_data.get("r", 999999),
            embedding=embedding,
            source="encrypted_lexicon"
        )
    
    def find_similar_words(self, word: str, count: int = 5) -> List[Tuple[str, float]]:
        """Find similar words using embeddings"""
        base_knowledge = self.lookup_word(word)
        if not base_knowledge or not base_knowledge.embedding:
            return []
        
        lemma = normalize_word(word)
        token = hmac.new(self._mac_key, lemma.encode("utf-8"), hashlib.sha256).hexdigest()
        shard_info = self._index.get(token)
        
        if not shard_info:
            return []
        
        # Search within the same shard for efficiency
        shard_data = self._get_shard(shard_info["sh"])
        similarities = []
        
        base_embedding = base_knowledge.embedding
        
        for other_lemma, other_data in shard_data.items():
            if other_lemma == lemma:
                continue
            
            if "m" in other_data:
                other_embedding = int8_dequantize(bytes(other_data["m"]))
                similarity = cosine_similarity(base_embedding, other_embedding)
                similarities.append((other_lemma, similarity))
        
        # Sort by similarity and return top results
        similarities.sort(key=lambda x: x[1], reverse=True)
        return similarities[:count]
    
    def _estimate_emotional_valence(self, word: str, definitions: List[str]) -> float:
        """Estimate emotional valence from word and definitions"""
        positive_indicators = [
            "good", "great", "wonderful", "amazing", "beautiful", "love", "joy", "happiness",
            "success", "win", "achieve", "accomplish", "heal", "comfort", "peace", "hope"
        ]
        negative_indicators = [
            "bad", "terrible", "awful", "hate", "fear", "anger", "sad", "pain", "hurt",
            "fail", "lose", "defeat", "suffer", "worry", "stress", "danger", "threat"
        ]
        
        text = (word + " " + " ".join(definitions)).lower()
        
        positive_score = sum(1 for indicator in positive_indicators if indicator in text)
        negative_score = sum(1 for indicator in negative_indicators if indicator in text)
        
        if positive_score + negative_score == 0:
            return 0.0
        
        return (positive_score - negative_score) / max(positive_score + negative_score, 1)
    
    def _estimate_arousal_level(self, word: str, definitions: List[str]) -> float:
        """Estimate arousal/energy level"""
        high_arousal = [
            "exciting", "intense", "extreme", "powerful", "strong", "loud", "fast", "quick",
            "energy", "action", "dynamic", "vibrant", "passionate", "fierce"
        ]
        
        text = (word + " " + " ".join(definitions)).lower()
        arousal_score = sum(1 for indicator in high_arousal if indicator in text)
        
        return min(arousal_score * 0.3, 1.0)
    
    def _estimate_concreteness(self, category: WordCategory, definitions: List[str]) -> float:
        """Estimate how concrete vs abstract the word is"""
        # Nouns tend to be more concrete
        if category == WordCategory.NOUN:
            base_score = 0.7
        elif category in [WordCategory.VERB, WordCategory.ADJECTIVE]:
            base_score = 0.5
        else:
            base_score = 0.3
        
        # Look for concrete indicators in definitions
        concrete_indicators = [
            "object", "thing", "physical", "material", "visible", "tangible", "touch", "see"
        ]
        abstract_indicators = [
            "concept", "idea", "theory", "principle", "philosophy", "meaning", "essence"
        ]
        
        text = " ".join(definitions).lower()
        concrete_score = sum(1 for indicator in concrete_indicators if indicator in text)
        abstract_score = sum(1 for indicator in abstract_indicators if indicator in text)
        
        if concrete_score > abstract_score:
            return min(base_score + 0.2, 1.0)
        elif abstract_score > concrete_score:
            return max(base_score - 0.3, 0.0)
        
        return base_score
    
    def _determine_semantic_domain(self, word: str, definitions: List[str], 
                                 category: WordCategory) -> SemanticDomain:
        """Determine the semantic domain of the word"""
        text = (word + " " + " ".join(definitions)).lower()
        
        domain_indicators = {
            SemanticDomain.EMOTIONAL: ["emotion", "feeling", "feel", "love", "hate", "fear", "joy"],
            SemanticDomain.THERAPEUTIC: ["therapy", "heal", "treatment", "recovery", "mental health"],
            SemanticDomain.PHILOSOPHICAL: ["philosophy", "meaning", "existence", "truth", "wisdom"],
            SemanticDomain.TECHNICAL: ["technology", "computer", "system", "technical", "engineering"],
            SemanticDomain.CREATIVE: ["art", "creative", "design", "imagination", "artistic"],
            SemanticDomain.SCIENTIFIC: ["science", "research", "study", "experiment", "theory"],
            SemanticDomain.SPIRITUAL: ["spiritual", "sacred", "divine", "soul", "meditation"],
            SemanticDomain.SOCIAL: ["social", "community", "relationship", "people", "society"]
        }
        
        for domain, indicators in domain_indicators.items():
            if any(indicator in text for indicator in indicators):
                return domain
        
        return SemanticDomain.GENERAL
    
    def get_statistics(self) -> Dict[str, Any]:
        """Get usage statistics"""
        total_lookups = self._stats["hits"] + self._stats["misses"]
        hit_rate = self._stats["hits"] / max(total_lookups, 1)
        
        return {
            "total_lookups": total_lookups,
            "cache_hits": self._stats["hits"],
            "cache_misses": self._stats["misses"],
            "hit_rate": hit_rate,
            "cache_size": len(self._cache),
            "index_size": len(self._index),
            "available": len(self._index) > 0
        }

# =============================================================================
# UNIVERSAL LANGUAGE CORE
# =============================================================================

class UniversalLanguageCore:
    """
    Universal language intelligence core that can be integrated with any AI system
    Provides massive vocabulary coverage and semantic understanding
    """
    
    def __init__(self, data_path: str, password: str = "universal_language", 
                 ai_adapter: Optional[UniversalAIAdapter] = None):
        self.data_path = Path(data_path)
        self.data_path.mkdir(parents=True, exist_ok=True)
        
        self.password = password
        self.ai_adapter = ai_adapter
        
        # Initialize encrypted lexicon
        self.encrypted_lexicon = UniversalEncryptedLexicon(
            str(self.data_path / "encrypted_lexicon"),
            password
        )
        
        # Custom vocabulary storage for AI-specific words
        self.custom_vocabulary: Dict[str, UniversalWordKnowledge] = {}
        
        # Processing cache
        self.word_cache: Dict[str, UniversalWordKnowledge] = {}
        self.context_cache: Dict[str, Dict[str, float]] = {}
        
        logger.info("Universal Language Core initialized")
    
    def add_custom_word(self, word_knowledge: UniversalWordKnowledge):
        """Add custom word knowledge specific to the AI system"""
        self.custom_vocabulary[word_knowledge.lemma] = word_knowledge
        # Clear cache entry if it exists
        if word_knowledge.lemma in self.word_cache:
            del self.word_cache[word_knowledge.lemma]
    
    def get_word_knowledge(self, word: str) -> Optional[UniversalWordKnowledge]:
        """Get comprehensive word knowledge with AI adaptation"""
        lemma = normalize_word(word)
        
        # Check cache first
        if lemma in self.word_cache:
            return self.word_cache[lemma]
        
        knowledge = None
        
        # Check custom vocabulary first (highest priority)
        if lemma in self.custom_vocabulary:
            knowledge = self.custom_vocabulary[lemma]
        
        # Fallback to encrypted lexicon
        if not knowledge:
            knowledge = self.encrypted_lexicon.lookup_word(word)
        
        # Adapt knowledge for the specific AI if adapter is available
        if knowledge and self.ai_adapter:
            # This allows the AI system to modify the knowledge for its needs
            adapted_knowledge = self.ai_adapter.process_word_knowledge(word, knowledge)
            if isinstance(adapted_knowledge, UniversalWordKnowledge):
                knowledge = adapted_knowledge
        
        # Cache the result
        if knowledge:
            self.word_cache[lemma] = knowledge
        
        return knowledge
    
    def find_related_words(self, word: str, count: int = 5, 
                          context: Optional[str] = None) -> List[UniversalWordKnowledge]:
        """Find words related to the given word"""
        word_knowledge = self.get_word_knowledge(word)
        if not word_knowledge:
            return []
        
        related_words = []
        
        # Add synonyms
        for synonym in word_knowledge.synonyms:
            syn_knowledge = self.get_word_knowledge(synonym)
            if syn_knowledge:
                related_words.append(syn_knowledge)
        
        # Add related words
        for related in word_knowledge.related_words:
            rel_knowledge = self.get_word_knowledge(related)
            if rel_knowledge:
                related_words.append(rel_knowledge)
        
        # Add similar words from embedding space
        try:
            similar_pairs = self.encrypted_lexicon.find_similar_words(word, count * 2)
            for similar_word, similarity in similar_pairs:
                if similarity > 0.3:  # Threshold for relevance
                    sim_knowledge = self.get_word_knowledge(similar_word)
                    if sim_knowledge and sim_knowledge not in related_words:
                        related_words.append(sim_knowledge)
        except Exception as e:
            logger.warning(f"Error finding similar words for '{word}': {e}")
        
        # Adapt selection based on AI personality and context
        if self.ai_adapter and context:
            context_needs = self.ai_adapter.estimate_context_needs(context)
            related_words = self.ai_adapter.adapt_vocabulary_selection(related_words, context_needs)
        
        return related_words[:count]
    
    def analyze_text_vocabulary(self, text: str) -> Dict[str, Any]:
        """Analyze the vocabulary complexity and characteristics of text"""
        words = re.findall(r'\b\w+\b', text.lower())
        unique_words = list(set(words))
        
        analysis = {
            "total_words": len(words),
            "unique_words": len(unique_words),
            "vocabulary_coverage": 0,
            "unknown_words": [],
            "semantic_domains": defaultdict(int),
            "complexity_distribution": defaultdict(int),
            "emotional_analysis": {"positive": 0, "negative": 0, "neutral": 0}
        }
        
        known_words = 0
        emotional_scores = []
        complexity_scores = []
        
        for word in unique_words:
            knowledge = self.get_word_knowledge(word)
            if knowledge:
                known_words += 1
                
                # Track semantic domains
                analysis["semantic_domains"][knowledge.semantic_domain.value] += 1
                
                # Track complexity
                if knowledge.complexity_level < 0.3:
                    analysis["complexity_distribution"]["simple"] += 1
                elif knowledge.complexity_level < 0.7:
                    analysis["complexity_distribution"]["moderate"] += 1
                else:
                    analysis["complexity_distribution"]["complex"] += 1
                
                # Track emotional valence
                if knowledge.emotional_valence > 0.3:
                    analysis["emotional_analysis"]["positive"] += 1
                elif knowledge.emotional_valence < -0.3:
                    analysis["emotional_analysis"]["negative"] += 1
                else:
                    analysis["emotional_analysis"]["neutral"] += 1
                
                emotional_scores.append(knowledge.emotional_valence)
                complexity_scores.append(knowledge.complexity_level)
            else:
                analysis["unknown_words"].append(word)
        
        analysis["vocabulary_coverage"] = known_words / max(len(unique_words), 1)
        analysis["average_emotional_valence"] = sum(emotional_scores) / max(len(emotional_scores), 1)
        analysis["average_complexity"] = sum(complexity_scores) / max(len(complexity_scores), 1)
        
        return analysis
    
    def suggest_vocabulary_for_context(self, context_description: str, count: int = 10) -> List[UniversalWordKnowledge]:
        """Suggest appropriate vocabulary for a given context"""
        if not self.ai_adapter:
            return []
        
        # Get context needs from AI adapter
        context_needs = self.ai_adapter.estimate_context_needs(context_description)
        
        # Find words that match the context needs
        suggested_words = []
        
        # Search through custom vocabulary first
        for word_knowledge in self.custom_vocabulary.values():
            score = self._calculate_context_relevance(word_knowledge, context_needs)
            if score > 0.5:
                suggested_words.append((word_knowledge, score))
        
        # Sort by relevance score
        suggested_words.sort(key=lambda x: x[1], reverse=True)
        
        return [word for word, score in suggested_words[:count]]
    
    def _calculate_context_relevance(self, word: UniversalWordKnowledge, 
                                   context_needs: Dict[str, float]) -> float:
        """Calculate how relevant a word is to the given context"""
        relevance = 0.0
        
        # Check semantic domain relevance
        domain_relevance = context_needs.get(word.semantic_domain.value, 0.0)
        relevance += domain_relevance * 0.4
        
        # Check emotional appropriateness
        if "emotional_positive" in context_needs and word.emotional_valence > 0:
            relevance += context_needs["emotional_positive"] * word.emotional_valence * 0.3
        elif "emotional_negative" in context_needs and word.emotional_valence < 0:
            relevance += context_needs["emotional_negative"] * abs(word.emotional_valence) * 0.3
        
        # Check complexity appropriateness
        if "complexity" in context_needs:
            complexity_match = 1.0 - abs(word.complexity_level - context_needs["complexity"])
            relevance += complexity_match * 0.2
        
        # Check formality appropriateness
        if "formality" in context_needs:
            formality_match = 1.0 - abs(word.formality_level - context_needs["formality"])
            relevance += formality_match * 0.1
        
        return min(relevance, 1.0)
    
    def get_comprehensive_stats(self) -> Dict[str, Any]:
        """Get comprehensive statistics about the language core"""
        lexicon_stats = self.encrypted_lexicon.get_statistics()
        
        return {
            "custom_vocabulary_size": len(self.custom_vocabulary),
            "word_cache_size": len(self.word_cache),
            "context_cache_size": len(self.context_cache),
            "encrypted_lexicon": lexicon_stats,
            "ai_adapter_active": self.ai_adapter is not None,
            "total_vocabulary_coverage": len(self.custom_vocabulary) + lexicon_stats.get("index_size", 0)
        }

# =============================================================================
# PRE-BUILT AI ADAPTERS
# =============================================================================

class GenericChatbotAdapter(UniversalAIAdapter):
    """Generic adapter for basic chatbot systems"""
    
    def __init__(self, name: str = "Generic Chatbot", personality_style: str = "neutral"):
        self.name = name
        self.personality_style = personality_style
        self._profile = self._create_generic_profile()
    
    def _create_generic_profile(self) -> AIPersonalityProfile:
        profile = AIPersonalityProfile(self.name)
        
        if self.personality_style == "friendly":
            profile.set_emotional_style("warm")
            profile.set_trait("friendliness", 0.8)
            profile.formality_preference = 0.3
        elif self.personality_style == "professional":
            profile.set_emotional_style("neutral")
            profile.formality_preference = 0.8
            profile.complexity_preference = 0.7
        elif self.personality_style == "casual":
            profile.set_emotional_style("energetic")
            profile.formality_preference = 0.2
            profile.complexity_preference = 0.3
        else:  # neutral
            profile.set_emotional_style("neutral")
            profile.formality_preference = 0.5
            profile.complexity_preference = 0.5
        
        return profile
    
    def get_personality_profile(self) -> AIPersonalityProfile:
        return self._profile
    
    def process_word_knowledge(self, word: str, knowledge: UniversalWordKnowledge) -> UniversalWordKnowledge:
        # Generic processing - just return as-is
        return knowledge
    
    def estimate_context_needs(self, text: str) -> Dict[str, float]:
        context = {}
        
        # Simple keyword-based context detection
        text_lower = text.lower()
        
        if any(word in text_lower for word in ["help", "problem", "issue", "trouble"]):
            context["therapeutic"] = 0.6
            context["emotional_positive"] = 0.4
        
        if any(word in text_lower for word in ["happy", "great", "wonderful", "amazing"]):
            context["emotional_positive"] = 0.8
        
        if any(word in text_lower for word in ["sad", "angry", "frustrated", "upset"]):
            context["emotional_negative"] = 0.7
            context["therapeutic"] = 0.5
        
        if any(word in text_lower for word in ["technical", "code", "system", "computer"]):
            context["technical"] = 0.8
            context["complexity"] = 0.7
        
        if any(word in text_lower for word in ["philosophy", "meaning", "purpose", "existence"]):
            context["philosophical"] = 0.8
            context["complexity"] = 0.8
        
        return context

class TherapeuticAIAdapter(UniversalAIAdapter):
    """Specialized adapter for therapeutic AI systems"""
    
    def __init__(self, name: str = "Therapeutic AI"):
        self.name = name
        self._profile = self._create_therapeutic_profile()
    
    def _create_therapeutic_profile(self) -> AIPersonalityProfile:
        profile = AIPersonalityProfile(self.name)
        
        # Therapeutic personality traits
        profile.set_trait("empathy", 0.95)
        profile.set_trait("compassion", 0.9)
        profile.set_trait("patience", 0.9)
        profile.set_trait("non_judgmental", 0.95)
        
        profile.set_emotional_style("empathetic")
        profile.formality_preference = 0.4  # Warm but professional
        profile.complexity_preference = 0.6  # Can handle complex emotions
        
        # Prefer therapeutic and emotional domains
        profile.add_preferred_domain(SemanticDomain.THERAPEUTIC)
        profile.add_preferred_domain(SemanticDomain.EMOTIONAL)
        profile.add_preferred_domain(SemanticDomain.PHILOSOPHICAL)
        
        # Add therapeutic vocabulary filters
        therapeutic_words = [
            "healing", "recovery", "growth", "support", "care", "comfort", "safety",
            "understanding", "validation", "acceptance", "strength", "resilience",
            "integration", "processing", "awareness", "mindfulness", "peace"
        ]
        profile.add_vocabulary_filter("therapeutic_preferred", therapeutic_words, 0.8)
        
        # Avoid potentially triggering words
        avoid_words = [
            "failure", "worthless", "hopeless", "broken", "damaged", "crazy",
            "weak", "pathetic", "useless", "stupid"
        ]
        profile.add_vocabulary_filter("potentially_harmful", avoid_words, -0.5)
        
        return profile
    
    def get_personality_profile(self) -> AIPersonalityProfile:
        return self._profile
    
    def process_word_knowledge(self, word: str, knowledge: UniversalWordKnowledge) -> UniversalWordKnowledge:
        # Enhance therapeutic relevance
        therapeutic_keywords = ["heal", "therapy", "recovery", "support", "care", "comfort"]
        
        if any(keyword in word.lower() or 
               keyword in " ".join(knowledge.definitions).lower() 
               for keyword in therapeutic_keywords):
            # Boost therapeutic domain classification
            knowledge.semantic_domain = SemanticDomain.THERAPEUTIC
            
            # Adjust emotional valence to be more positive for therapeutic terms
            if knowledge.emotional_valence < 0.2:
                knowledge.emotional_valence = max(knowledge.emotional_valence, 0.2)
        
        return knowledge
    
    def estimate_context_needs(self, text: str) -> Dict[str, float]:
        context = {}
        text_lower = text.lower()
        
        # Crisis indicators
        crisis_words = ["suicide", "kill myself", "end it", "can't go on", "hopeless"]
        if any(word in text_lower for word in crisis_words):
            context["therapeutic"] = 1.0
            context["emotional_positive"] = 0.9
            context["complexity"] = 0.3  # Keep it simple in crisis
        
        # Emotional distress
        distress_words = ["depressed", "anxious", "panic", "overwhelmed", "trauma"]
        if any(word in text_lower for word in distress_words):
            context["therapeutic"] = 0.9
            context["emotional_positive"] = 0.7
        
        # Healing/growth context
        growth_words = ["healing", "recovery", "growth", "therapy", "better"]
        if any(word in text_lower for word in growth_words):
            context["therapeutic"] = 0.8
            context["emotional_positive"] = 0.6
        
        # Relationship issues
        relationship_words = ["relationship", "partner", "family", "friend", "conflict"]
        if any(word in text_lower for word in relationship_words):
            context["social"] = 0.7
            context["therapeutic"] = 0.6
        
        return context

class CreativeAIAdapter(UniversalAIAdapter):
    """Specialized adapter for creative AI systems"""
    
    def __init__(self, name: str = "Creative AI"):
        self.name = name
        self._profile = self._create_creative_profile()
    
    def _create_creative_profile(self) -> AIPersonalityProfile:
        profile = AIPersonalityProfile(self.name)
        
        # Creative personality traits
        profile.set_trait("creativity", 0.95)
        profile.set_trait("imagination", 0.9)
        profile.set_trait("expressiveness", 0.85)
        profile.set_trait("openness", 0.9)
        
        profile.set_emotional_style("energetic")
        profile.formality_preference = 0.3  # More casual and expressive
        profile.complexity_preference = 0.7  # Can handle complex creative concepts
        
        # Prefer creative and artistic domains
        profile.add_preferred_domain(SemanticDomain.CREATIVE)
        profile.add_preferred_domain(SemanticDomain.EMOTIONAL)
        
        # Add creative vocabulary filters
        creative_words = [
            "imagination", "inspiration", "artistic", "creative", "expression", "beauty",
            "aesthetic", "design", "color", "rhythm", "harmony", "composition",
            "vision", "dream", "fantasy", "innovation", "original", "unique"
        ]
        profile.add_vocabulary_filter("creative_preferred", creative_words, 0.9)
        
        return profile
    
    def get_personality_profile(self) -> AIPersonalityProfile:
        return self._profile
    
    def process_word_knowledge(self, word: str, knowledge: UniversalWordKnowledge) -> UniversalWordKnowledge:
        # Enhance creative relevance
        creative_keywords = ["art", "create", "design", "imagine", "express", "beauty"]
        
        if any(keyword in word.lower() or 
               keyword in " ".join(knowledge.definitions).lower() 
               for keyword in creative_keywords):
            knowledge.semantic_domain = SemanticDomain.CREATIVE
            
            # Boost emotional valence for creative terms
            if knowledge.emotional_valence < 0.3:
                knowledge.emotional_valence = min(knowledge.emotional_valence + 0.2, 1.0)
            
            # Increase arousal level for creative terms
            knowledge.arousal_level = min(knowledge.arousal_level + 0.3, 1.0)
        
        return knowledge
    
    def estimate_context_needs(self, text: str) -> Dict[str, float]:
        context = {}
        text_lower = text.lower()
        
        # Creative project context
        creative_words = ["create", "design", "art", "write", "compose", "draw", "paint"]
        if any(word in text_lower for word in creative_words):
            context["creative"] = 0.9
            context["emotional_positive"] = 0.6
            context["complexity"] = 0.7
        
        # Inspiration seeking
        inspiration_words = ["inspiration", "ideas", "brainstorm", "imagine"]
        if any(word in text_lower for word in inspiration_words):
            context["creative"] = 0.8
            context["emotional_positive"] = 0.7
        
        # Technical creative work
        technical_creative = ["software", "code", "programming", "algorithm", "digital"]
        if any(word in text_lower for word in technical_creative):
            context["creative"] = 0.6
            context["technical"] = 0.7
        
        return context

# =============================================================================
# EASY INTEGRATION HELPERS
# =============================================================================

class LanguageCoreFactory:
    """Factory for creating language cores with different AI adapters"""
    
    @staticmethod
    def create_for_chatbot(data_path: str, password: str = "chatbot_language", 
                          style: str = "friendly") -> UniversalLanguageCore:
        """Create language core for a generic chatbot"""
        adapter = GenericChatbotAdapter("Chatbot", style)
        return UniversalLanguageCore(data_path, password, adapter)
    
    @staticmethod
    def create_for_therapy(data_path: str, password: str = "therapy_language") -> UniversalLanguageCore:
        """Create language core for therapeutic AI"""
        adapter = TherapeuticAIAdapter("TherapyBot")
        return UniversalLanguageCore(data_path, password, adapter)
    
    @staticmethod
    def create_for_creativity(data_path: str, password: str = "creative_language") -> UniversalLanguageCore:
        """Create language core for creative AI"""
        adapter = CreativeAIAdapter("CreativeAI")
        return UniversalLanguageCore(data_path, password, adapter)
    
    @staticmethod
    def create_universal(data_path: str, password: str = "universal_language") -> UniversalLanguageCore:
        """Create generic language core without AI-specific adaptation"""
        return UniversalLanguageCore(data_path, password, None)

# =============================================================================
# SIMPLE API FOR ANY AI SYSTEM
# =============================================================================

class SimpleLanguageAPI:
    """Simplified API for easy integration with any AI system"""
    
    def __init__(self, data_path: str, ai_type: str = "generic", password: str = "language_core"):
        """
        Initialize simple language API
        
        Args:
            data_path: Path to store language data
            ai_type: Type of AI ("generic", "therapeutic", "creative", "chatbot")
            password: Password for encrypted lexicon
        """
        
        if ai_type == "therapeutic":
            self.core = LanguageCoreFactory.create_for_therapy(data_path, password)
        elif ai_type == "creative":
            self.core = LanguageCoreFactory.create_for_creativity(data_path, password)
        elif ai_type == "chatbot":
            self.core = LanguageCoreFactory.create_for_chatbot(data_path, password)
        else:
            self.core = LanguageCoreFactory.create_universal(data_path, password)
    
    def understand_word(self, word: str) -> Dict[str, Any]:
        """Get simple understanding of a word"""
        knowledge = self.core.get_word_knowledge(word)
        if not knowledge:
            return {"found": False, "word": word}
        
        return {
            "found": True,
            "word": knowledge.word,
            "definitions": knowledge.definitions,
            "synonyms": knowledge.synonyms,
            "category": knowledge.category.value,
            "emotional_valence": knowledge.emotional_valence,
            "complexity": knowledge.complexity_level,
            "domain": knowledge.semantic_domain.value
        }
    
    def find_similar_words(self, word: str, count: int = 5) -> List[Dict[str, Any]]:
        """Find words similar to the given word"""
        related = self.core.find_related_words(word, count)
        return [
            {
                "word": w.word,
                "definitions": w.definitions[:1],  # Just first definition
                "similarity_reason": "synonym" if w.word in self.understand_word(word).get("synonyms", []) else "semantic"
            }
            for w in related
        ]
    
    def analyze_text(self, text: str) -> Dict[str, Any]:
        """Analyze text vocabulary and characteristics"""
        return self.core.analyze_text_vocabulary(text)
    
    def suggest_words_for_context(self, context: str, count: int = 10) -> List[Dict[str, Any]]:
        """Suggest appropriate words for a context"""
        suggestions = self.core.suggest_vocabulary_for_context(context, count)
        return [
            {
                "word": w.word,
                "definition": w.definitions[0] if w.definitions else "",
                "why_suggested": f"Good for {w.semantic_domain.value} context"
            }
            for w in suggestions
        ]
    
    def add_custom_word(self, word: str, definition: str, category: str = "noun", 
                       emotional_valence: float = 0.0, **kwargs):
        """Add a custom word to the AI's vocabulary"""
        
        # Map category string to enum
        category_map = {
            "noun": WordCategory.NOUN,
            "verb": WordCategory.VERB,
            "adjective": WordCategory.ADJECTIVE,
            "adverb": WordCategory.ADVERB
        }
        
        word_knowledge = UniversalWordKnowledge(
            word=word,
            lemma=normalize_word(word),
            category=category_map.get(category, WordCategory.NOUN),
            definitions=[definition],
            emotional_valence=emotional_valence,
            **kwargs
        )
        
        self.core.add_custom_word(word_knowledge)
    
    def get_stats(self) -> Dict[str, Any]:
        """Get language core statistics"""
        return self.core.get_comprehensive_stats()

# =============================================================================
# USAGE EXAMPLES AND DOCUMENTATION
# =============================================================================

def example_usage():
    """Example of how to use the Universal Language Core with different AI types"""
    
    print(" Universal Encrypted Language Core Examples")
    print("=" * 50)
    
    # Example 1: Simple chatbot integration
    print("\n1. Simple Chatbot Integration:")
    chatbot_api = SimpleLanguageAPI("./chatbot_data", "chatbot")
    
    word_info = chatbot_api.understand_word("happiness")
    print(f"Understanding 'happiness': {word_info}")
    
    similar = chatbot_api.find_similar_words("joy", 3)
    print(f"Words similar to 'joy': {similar}")
    
    # Example 2: Therapeutic AI integration
    print("\n2. Therapeutic AI Integration:")
    therapy_api = SimpleLanguageAPI("./therapy_data", "therapeutic")
    
    # Add custom therapeutic vocabulary
    therapy_api.add_custom_word(
        "grounding", 
        "A therapeutic technique to stay present and connected to reality",
        category="noun",
        emotional_valence=0.6
    )
    
    analysis = therapy_api.analyze_text("I'm feeling overwhelmed and need support")
    print(f"Text analysis: {analysis}")
    
    # Example 3: Creative AI integration
    print("\n3. Creative AI Integration:")
    creative_api = SimpleLanguageAPI("./creative_data", "creative")
    
    suggestions = creative_api.suggest_words_for_context("writing a poem about nature")
    print(f"Word suggestions for poetry: {suggestions}")
    
    # Example 4: Custom AI adapter
    print("\n4. Custom AI Adapter:")
    
    class CustomAIAdapter(UniversalAIAdapter):
        def get_personality_profile(self):
            profile = AIPersonalityProfile("My Custom AI")
            profile.set_emotional_style("analytical")
            profile.formality_preference = 0.8
            return profile
        
        def process_word_knowledge(self, word, knowledge):
            # Custom processing logic
            return knowledge
        
        def estimate_context_needs(self, text):
            # Custom context analysis
            return {"technical": 0.8, "complexity": 0.9}
    
    # Create custom language core
    custom_adapter = CustomAIAdapter()
    custom_core = UniversalLanguageCore("./custom_data", "custom_password", custom_adapter)
    
    print("Custom AI language core created!")
    
    # Show statistics
    stats = chatbot_api.get_stats()
    print(f"\nLanguage Core Statistics: {stats}")

if __name__ == "__main__":
    # Run examples
    example_usage()

"""
UNIVERSAL ENCRYPTED LANGUAGE CORE INTEGRATION GUIDE
==================================================

## Quick Start for Any AI System:

### 1. Simple Integration (5 lines of code):
```python
from universal_language_core import SimpleLanguageAPI

# Initialize for your AI type
language_api = SimpleLanguageAPI("./my_ai_data", ai_type="chatbot")

# Use immediately
word_info = language_api.understand_word("consciousness")
similar_words = language_api.find_similar_words("healing", 5)
text_analysis = language_api.analyze_text("Your text here")
```

### 2. Supported AI Types:
- **"chatbot"** - General conversational AI
- **"therapeutic"** - Mental health and therapy bots
- **"creative"** - Creative writing and artistic AI
- **"generic"** - Universal compatibility

### 3. Core Features:
-  **Massive Vocabulary**: 100k+ encrypted English words
-  **Semantic Understanding**: Definitions, synonyms, relationships
-  **AI Personality Adaptation**: Vocabulary filtered for your AI's style
-  **Complete Privacy**: Encrypted, offline-only operation
-  **High Performance**: Lazy loading, LRU caching, optimized access
-  **Easy Integration**: Simple API, plug-and-play design

### 4. Advanced Custom Integration:
```python
class MyAIAdapter(UniversalAIAdapter):
    def get_personality_profile(self):
        # Define your AI's personality
        pass
    
    def process_word_knowledge(self, word, knowledge):
        # Customize word understanding for your AI
        pass
    
    def estimate_context_needs(self, text):
        # Analyze what vocabulary your AI needs
        pass

core = UniversalLanguageCore("./data", "password", MyAIAdapter())
```

### 5. Building the Encrypted Lexicon:
```python
# One-time setup to build the encrypted vocabulary database
from encrypted_lexicon_builder import EncryptedLexiconBuilder

builder = EncryptedLexiconBuilder()
builder.add_many(your_vocabulary_source())  # Your word data
builder.build("./language_data/encrypted_lexicon", "your_password")
```

## Benefits:
 **No LLM Dependency** - Rich vocabulary without external AI
 **Complete Privacy** - All processing offline and encrypted  
 **Universal Compatibility** - Works with any AI architecture
 **Personality Aware** - Adapts vocabulary to your AI's style
 **Production Ready** - Optimized for real-world deployment
 **Easy Integration** - 5-minute setup for most AI systems

This system transforms any AI from basic vocabulary to sophisticated
linguistic intelligence while maintaining complete privacy and control.
"""